{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 2: PyTorch Basic Exercise**\n",
    "\n",
    "### **TAs: Chiku Parida (chipa@dtu.dk), Dr. Dipendu Roy (dipro@dtu.dk)**\n",
    "\n",
    "## **Objectives:**\n",
    "- Understand the basics of PyTorch tensors and operations.\n",
    "- Implement a simple neural network using PyTorch.\n",
    "- Train and evaluate the neural network on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import dependencies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "\n",
    "# Data parameters\n",
    "n_train, n_val, n_test = 20, 20, 20\n",
    "a = 2.5  # Frequency of the first sine wave\n",
    "b = 7.0  # Frequency of the second sine wave\n",
    "c = 1.0  # Mixing parameter, between 0 and 1\n",
    "noise = 0.1  # Noise level, greater than 0\n",
    "train_x_scale = 1.0  # Scale x range for the train data\n",
    "val_x_scale = 1.0  # Scale x range for the val data\n",
    "test_x_scale = 1.1  # Scale x range for the test data\n",
    "\n",
    "# Define true function\n",
    "def f(x):\n",
    "    return c * torch.sin(a * x) + (1 - c) * torch.sin(b * x)\n",
    "\n",
    "# Define function to generate data with noise\n",
    "def generate_xy(n, noise=0.1, x_scale=1.0):\n",
    "    x = torch.randn(n) * x_scale\n",
    "    y = f(x) + torch.randn(n) * noise\n",
    "    return x, y\n",
    "\n",
    "# Generate training, validation and test data\n",
    "x = torch.linspace(-4, 4, 1000)\n",
    "x_train, y_train = generate_xy(n_train)\n",
    "x_val, y_val = generate_xy(n_val)\n",
    "x_test, y_test = generate_xy(n_test, noise, test_x_scale)\n",
    "\n",
    "\n",
    "# Define function for plotting the data\n",
    "def plot(x_pred=None, y_pred=None):\n",
    "    plt.figure()\n",
    "    plt.title(\"Data\")\n",
    "    plt.plot(x, f(x), color=\"k\", label='true')\n",
    "    plt.scatter(x_train, y_train, label='train')\n",
    "    plt.scatter(x_val, y_val, label='val')\n",
    "    plt.scatter(x_test, y_test, label='test')\n",
    "    # plt.xlim(-4, 4)\n",
    "    if x_pred is not None and y_pred is not None:\n",
    "        plt.plot(x_pred, y_pred, color=\"r\", label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the data\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=10, act=torch.relu):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(1, hidden_size)\n",
    "        self.act = act\n",
    "        self.output = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)  # hidden layer\n",
    "        x = self.act(x)  # activation function\n",
    "        x = self.output(x)  # output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with an untrained model\n",
    "\n",
    "# Instantiate untrained model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Print the model\n",
    "print(model)\n",
    "\n",
    "# Predict with the untrained model\n",
    "y_pred = model(x.unsqueeze(1)).squeeze().detach()\n",
    "\n",
    "# Plot the data and the predictions\n",
    "plot(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def train(model, x_train, y_train, x_val, y_val, epochs=1000, lr=0.01, weight_decay=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()  # Loss function\n",
    "    records = []\n",
    "    for epoch in range(epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train.unsqueeze(1)).squeeze()\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        records.append({\"epoch\": epoch, \"train_loss\": loss.item()})\n",
    "        if epoch == 0 or epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            y_val_pred = model(x_val.unsqueeze(1)).squeeze()\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "            records.append({\"epoch\": epoch, \"val_loss\": loss.item()})\n",
    "            print(f\"Epoch {epoch:5d} train_loss: {loss.item():.4f} val_loss: {val_loss.item():.4f}\")\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "# Model and training parameters\n",
    "hidden_size = 10\n",
    "activation_function = torch.sigmoid  # torch.sigmoid, torch.relu, torch.nn.SiLU()\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0  # L2 penalty, between 0.0 and 0.1\n",
    "\n",
    "# Plot activation function\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.title(\"Activation function\")\n",
    "plt.plot(x, activation_function(x))\n",
    "plt.show()\n",
    "\n",
    "# Instantiate and train a new model\n",
    "model = NeuralNetwork(hidden_size, activation_function)\n",
    "log = train(model, x_train, y_train, x_val, y_val, epochs, learning_rate, weight_decay)\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure()\n",
    "plt.title(\"Training curve\")\n",
    "for k in [\"train_loss\", \"val_loss\"]:\n",
    "    tmp = log[[\"epoch\", k]].dropna()\n",
    "    plt.plot(tmp[\"epoch\"], tmp[k], label=k)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with trained model\n",
    "\n",
    "y_pred = model(x.unsqueeze(1)).squeeze().detach()\n",
    "\n",
    "plot(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "y_test_pred = model(x_test.unsqueeze(1)).squeeze().detach()\n",
    "\n",
    "# Compute the mean absolute error on the test data\n",
    "mae = torch.mean(torch.abs(y_test - y_test_pred))\n",
    "# Compute the mean squared error on the test data\n",
    "mse = torch.mean(torch.square(y_test - y_test_pred))\n",
    "\n",
    "lim = (-1.2, 1.2)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Parity plot\")\n",
    "plt.scatter(y_test, y_test_pred, color=\"C2\", label=f\"MAE={mae:.4f}, MSE={mse:.4f}\")\n",
    "plt.plot(lim, lim, 'k--')\n",
    "plt.xlabel(\"true\"); plt.ylabel(\"pred\")\n",
    "plt.xlim(lim); plt.ylim(lim)\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.title(\"Residual plot\")\n",
    "plt.scatter(y_test, y_test - y_test_pred, color=\"C2\", label=f\"MAE={mae:.4f}, MSE={mse:.4f}\")\n",
    "plt.plot(lim, [0, 0], 'k--')\n",
    "plt.xlabel(\"true\"); plt.ylabel(\"true - pred\")\n",
    "plt.xlim(lim); plt.ylim(lim)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Read and understand the code. Especially the model and training procedure. Refer to the [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
    "* Try to change the data parameters to make the task harder. For example try `c=0.5`.\n",
    "* Then try to change the model and training parameters to improve the predictions. For example increase the size of the hidden layer and the amount of training data.\n",
    "* Try a different activation function. \n",
    "* Add another hidden layer to the model.\n",
    "* Make your own experiments!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
